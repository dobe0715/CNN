{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMukCh1+sMXH7HiIEMpItgl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Residual Dense Network Design"],"metadata":{"id":"Xwr1f6o5yjL6"}},{"cell_type":"markdown","source":["## (Paper review) DenseNets Reloaded:Paradigm Shift Beyond ResNets and ViTs(2024)"],"metadata":{"id":"OwBlORUlylcn"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1lpoyy09PHwjF0porrGiRB-TpItSqSltA\" height=350>"],"metadata":{"id":"jo83-g5D1Ofr"}},{"cell_type":"markdown","source":["+ 2015년 처음 등장한 residual connection은 엄청난 성능을 보였고, 이후의 여러 application에 빠지지 않는 요소이다."],"metadata":{"id":"-oNUMcpiylaY"}},{"cell_type":"markdown","source":["+ 2016년, addition이 아니라, concatenation을 사용한 dense connection이 등장했고, residual connection보다 뛰어난 성능을 보였다."],"metadata":{"id":"vusjZeA7ylYR"}},{"cell_type":"markdown","source":["+ __하지만, memory소모, channel(width) scaling 문제 등으로 인해 residual connection에 비해 많이 사용되지 않았다.__"],"metadata":{"id":"Yt2CQiliylV2"}},{"cell_type":"markdown","source":["### Residual connection"],"metadata":{"id":"7ThKSZldylTe"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1MwfpDLr4Wf7nyA4c2_eDCuQC2ig4sWo4\" width=350>"],"metadata":{"id":"AXaulevtylRI"}},{"cell_type":"markdown","source":["#### 장점1 : 학습이 쉬워진다."],"metadata":{"id":"XhWkWfrtylOn"}},{"cell_type":"markdown","source":["network가 아주 깊어지면, layer를 지날 때, 입력으로부터 크게 변하지 않는 것이 이상적이다."],"metadata":{"id":"7zTOtzP7ylMZ"}},{"cell_type":"markdown","source":["즉,   \n","$H(x) = F(x) + x$  \n","$H(x) \\simeq x$  \n","따라서,   \n","$F(x) \\simeq 0$"],"metadata":{"id":"XBJB6jo1ylJx"}},{"cell_type":"markdown","source":["이는 우리가 학습하고자 하는 함수 F(x)를 0으로부터 조금씩만 바꿔나가면 되므로, 학습이 용이해진다."],"metadata":{"id":"46KF8cUEylHM"}},{"cell_type":"markdown","source":["#### 장점2 : vanishing gradient 문제 해결."],"metadata":{"id":"buSWOljIylEv"}},{"cell_type":"markdown","source":["위의 식에서 $H(x)$에서 $x$에 대한 편미분값을 생각해보면, $\\cfrac{\\partial{F(x)}}{\\partial{x}} + 1$이다. 항상 1보다 크므로, 충분한 크기의 gradient가 흐르게된다."],"metadata":{"id":"IKBjrnLNylCK"}},{"cell_type":"markdown","source":["### Dense connection"],"metadata":{"id":"jLfHx5Fxyk_h"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=12R2_GtZCzgfaUuvBwcD5UOVYr9tot5dN\" height=300>"],"metadata":{"id":"iSBwXDfVyk8x"}},{"cell_type":"markdown","source":["이전 layer에서의 결과를 다음 layer 결과에 그대로 concat"],"metadata":{"id":"sWmBZcMLyk6M"}},{"cell_type":"markdown","source":["#### 장점1 : feature를 낭비 없이 재사용한다."],"metadata":{"id":"y0c1T_jlyk3y"}},{"cell_type":"markdown","source":["#### 장점2(논문에서의 추측) : rank를 높여준다."],"metadata":{"id":"U3QftX8Kyk1A"}},{"cell_type":"markdown","source":["$f(XW)$  \n","+ $W \\in \\mathbb{R}^{d_{in} \\times d_{out}}$ : layer\n","+ $X \\in \\mathbb{R}^{N \\times d_{in}}$ : input feature\n","+ $f$ : activation(nonlinearity function)\n","+ N : number of data"],"metadata":{"id":"kk08Gkiwykyd"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1Wby4Qlx3x_LRHp8Prk54LPHCQKg5yEaT\" height=150>"],"metadata":{"id":"r6zmppoACyQy"}},{"cell_type":"markdown","source":["그러면, $rank(f(XW)) \\simeq d_{out}$일 것이다!"],"metadata":{"id":"lGW5PQWfCyNV"}},{"cell_type":"markdown","source":["$\\because f$ : nonlinearity function"],"metadata":{"id":"k6M7K8EmCyLH"}},{"cell_type":"markdown","source":["따라서, 여기서의 XW를 concate함으로 표현해버리면, 원래의 X보다 더 큰 차원의 representative를 갖게된다."],"metadata":{"id":"B23_Gp-vCyJY"}},{"cell_type":"markdown","source":["#### Modernizing DenseNet"],"metadata":{"id":"CmalruUjFxJ6"}},{"cell_type":"markdown","source":["그러던 와중, 자연어처리에 사용되는 모델인 Transformer를 응용한 ViT기반 모델이 큰 성능향상을 보였다. 하지만, swin transformer등의 모델으로 발전하면서, 다시금 Conv layer의 spatial한 정보처리가 중요해지고, 이전으로 돌아가게 되는중이다."],"metadata":{"id":"MYrLzzXAFKhA"}},{"cell_type":"markdown","source":["그러한 논문의 대표 주자 격인 모델이 ConvNeXt이다. (기존의 ResNet을 현대화하여 conv만 사용해 sota를 달성한 모델)"],"metadata":{"id":"CikqKIuSF_rL"}},{"cell_type":"markdown","source":["기존에 ConvNeXt와 같이 DenseNet을 현대적인 기법으로 Modernize해주었다."],"metadata":{"id":"Lu2fpPAnE4UI"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1hg_-NyvAcQtkePzU_VZS9mwRJ2XwJIpr\" height=300>"],"metadata":{"id":"MRNAK_a9CyG2"}},{"cell_type":"markdown","source":["구조는 아래의 그림 하나를 block으로 하여,   \n","(3, 3, 12, 3)으로 4개의 stage를 잡고, stage마다 channel 커지고, down sampling해주는 구조이다."],"metadata":{"id":"WUghEQzgIBBo"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1VHtR1PW4qX5qEWH7oh7alm5kWlPfySKB\" height=200>"],"metadata":{"id":"Rxbd527-CyEM"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1pX-_yhtH8nL6RWPaWOsoLYGYS0ehS15k\n","\" height=300>"],"metadata":{"id":"HE1N4SyCCyB0"}},{"cell_type":"markdown","source":["모든 부분에서 concat(dense)가 add(residual)보다 앞섰다고는 하는데..."],"metadata":{"id":"bba3jMQJCx_b"}},{"cell_type":"markdown","source":["__나의 생각__  \n","residual connection과 dense connection은 애초에 서로 다른 영역에서의 효과를 갖는다!!"],"metadata":{"id":"WQtkEmkpJr2N"}},{"cell_type":"markdown","source":["+ residual connection : 중간의 함수 F의 학습을 용이하게 해주고, gradient vanishing을 막아주는 역할.\n","+ dense connection : 이전 feature의 정보 손실을 막고 이후에 재사용하며 rank를 높여 모델의 표현력 증가 역할."],"metadata":{"id":"iPUBmKENJrzF"}},{"cell_type":"markdown","source":["__따라서, 하나만 쓸게 아니라 둘 다 사용해보자!!__"],"metadata":{"id":"k5mOlTowJrw0"}},{"cell_type":"markdown","source":["### ResDense connection"],"metadata":{"id":"CT4hd2exK0RU"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1wyzMaiTRC94-QuOlDEgr-pIAT5fkFfXg\" height=600>"],"metadata":{"id":"_dDBw2KOJrsC"}}]}